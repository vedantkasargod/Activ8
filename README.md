# Activ8
A tool for neural network developers that allows combining multiple activation functions. For my trial and error neural net brothers, the accuracy enhancing hunt has a new tool in inventory.


# Custom Activation Functions for PyTorch

This project contains various custom activation functions for neural networks implemented in PyTorch, including:
- Piecewise Linear Activation
- Exponential Activation
- Softplus
- Softmax
- ReLU
- LeakyReLU
- GELU
- ELU
- PReLU
- Maxout

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/activation-functions.git
   cd activation-functions

2. Install dependencies
   ```bash
   pip install -r requirements.txt
